apiVersion: v1
kind: Namespace
metadata:
  name: firekube-example
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-serve
  namespace: firekube-example
  labels:
    app: model-serve
spec:
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  revisionHistoryLimit: 5
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  selector:
    matchLabels:
      app: model-serve
  template:
    metadata:
      annotations:
        prometheus.io/port: "8080"
        prometheus.io/scrape: "true"
      labels:
        app: model-serve
    spec:
      initContainers:
      - name: model-container
        image: chanwit/titanic-model:v2
        args:
        - /model.joblib
        - /models/model.joblib
        volumeMounts:
        - name: model-data
          mountPath: /models
      containers:
      - name: model-serve
        image: chanwit/kfserving-sklearnserver:v0.2.2c
        args:
        - --http_port
        - "8080"
        - --model_dir
        - /models
        - --model_name
        - titanic
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: model-data
          mountPath: /models
      volumes:
      - name: model-data
        hostPath:
          path: /tmp/model-serve
---
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: model-serve
  namespace: firekube-example
spec:
  maxReplicas: 4
  metrics:
  - resource:
      name: cpu
      targetAverageUtilization: 99
    type: Resource
  minReplicas: 2
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-serve
---
apiVersion: flagger.app/v1alpha3
kind: Canary
metadata:
  name: model-serve
  namespace: firekube-example
spec:
  # deployment reference
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-serve
  # the maximum time in seconds for the canary deployment
  # to make progress before it is rollback (default 600s)
  progressDeadlineSeconds: 120
  # HPA reference (optional)
  autoscalerRef:
    apiVersion: autoscaling/v2beta1
    kind: HorizontalPodAutoscaler
    name: model-serve
  service:
    # service port number
    port: 8080
    # container port number or name (optional)
    targetPort: 8080
    # Istio traffic policy (optional)
    trafficPolicy:
      tls:
        # use ISTIO_MUTUAL when mTLS is enabled
        mode: DISABLE
    # Istio retry policy (optional)
    retries:
      attempts: 3
      perTryTimeout: 1s
      retryOn: "gateway-error,connect-failure,refused-stream"
  canaryAnalysis:
    # schedule interval (default 60s)
    interval: 1m
    # max number of failed metric checks before rollback
    threshold: 5
    # max traffic percentage routed to canary
    # percentage (0-100)
    maxWeight: 50
    # canary increment step
    # percentage (0-100)
    stepWeight: 10
    webhooks:
      - name: acceptance-test
        type: pre-rollout
        url: http://flagger-loadtester.firekube-example/
        timeout: 30s
        metadata:
          type: bash
          cmd: "curl -s http://model-serve-canary.firekube-example:8080/v1/models/titanic | grep '\"ready\": true'"
      - name: load-test
        url: http://flagger-loadtester.firekube-example/
        timeout: 5s
        metadata:
          cmd: "hey -z 1m -q 10 -c 2 http://model-serve-canary.firekube-example:8080/v1/models/titanic"
